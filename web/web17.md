

# Caching(캐싱)

**본 내용은 10분 테코톡 파피님의 강의를 토대로 작성하였습니다.**



## 1. Cache

* 오늘날 캐싱은 하드웨어 수준에서 그치는 것이 아니 운영체제, CNS, DNS등의 네트워킹 계층 그리고 웹 애플리케이션 및 데이터베이스를 비롯한 다양한 기술 계층에 걸쳐 적용되고 활용되고 있음
* 물건을 일시적으로 저장, 보관하기 위해 사용하는 곳이라는 의미
* 자주 필요한 데이터나 값을 일시적으로 저장
* Caching: Cache를 사용하는 것



## 2. 캐시메모리

### 컴퓨터의 동작 흐름

> **CPU <-> RAM <-> Hard Drive**

* CPU: 중앙처리장치
  * 매우 빠름
  * 기억장치에서 데이터를 받아들여 연산 작업
* RAM: 주기억장치
  * 빠름
  * 전원이 꺼지면 데이터가 지워짐
* Hard Drive: 보조기억장치
  * 매우 느림
  * 전원이 꺼져도 데이터가 지워지지 않음



### CPU와 메모리 간 성능 차이

* 기술이 발전할 수록 CPU와 메모리의 속도의 차이가 커짐
  * 메모리는 속도보다는 용량을 증가시키는 방향으로 발전
* CPU는 데이터 처리를 위해 메모리와 끊임없이 데이터를 주고받는 구조
* 메모리가 CPU의 데이터 처리 속도를 쫓아가지 못함
* CPU가 메모리를 기다려야 하는 병목현상 발생
* 이 병목 현상을 완화하기 위해서 캐시 메모리를 도입



### 캐시 메모리 도입

> **CPU <-> Cache Memory <-> RAM <-> Hard Drive**

* Cache Memory
  * 크기는 작지만 속도가 빠름
  * 재사용 가능성이 높은 데이터 복사본을 저장해둔 후 CPU가 요청하는 데이터를 바로바로 전달
  * 더 복잡하고 비싼 구조를 요구
  * 캐싱을 이용하여 빠르고 작은 메모리와 크고 느린 메모리의 장점을 조합해 크코 빠른 메모리처럼 행동하도록 만듦



## 3. 캐시 동작 원리

### 데이터 지역성의 원리

* 데이터 접근이 시간적 혹은 공간적으로 가깝게 일어나는 것
* 한 번 참조된 변수는 잠시 후에 또 참조될 가능성이 높다.
* 어떤 데이터에 접근할 때, 그 데이터 근처에 있는 다른 데이터도 참조될 가능성이 높다.
* 시간 지역성
  * 특정 데이터가 한 번 접근되었을 경우, 가까운 미래에 또 한번 데이터에 접근할 가능성이 높음
  * 메모리 상의 같은 주소에 여러 차례 읽기 쓰기를 수행할 경우, 상대적으로 작은 크기의 캐시를 사용해도 효율성을 높일 수 있음
    * for, while 에서의 변수
* 공간 지역성
  * 특정 데이터와 가까운 주소가 순서대로 접근되는 경우
  * 한 메모리 주소에 접근할 때 그 주소뿐 아니라 해당 블록을 전부 캐시에 가져옴
  * 이때 메모리 주소를 오름차순이나 내림차순으로 접근한다면, 캐시에 이미 저장된 같은 불록의 데이터를 접근하게 되므로 캐시의 효율성이 크게 향상
    * 배열



### 캐시 히트와 캐시 미스

* 캐시 히트: 캐시 메모리가 해당 데이터를 가지고 있는 것(캐시 적중)
* 캐시 미스: 해당 데이터가 없어서 메인 메모리에서 가져와야 하는 경우



### 캐시 메모리 쓰기 정책과 캐시 일관성

* CPU에서 데이터를 읽는 동작이 아니라, 입력하는 동작이 발생하고 데이터를 변경할 주소가 캐싱된 상태라면 메모리의 데이터가 업데이트 되는 대신 캐시의 데이터가 업데이트
  * 이에 따라서 메인 메모리를 업데이트
* Write Through 정책
  * 메인 메모리를 바로 업데이트
  * 단순하고 캐시와 메인 메모리의 일관성을 유지
  * 매번 바꿔야 하므로 느림
* Write Back 정책
  * 캐시만 업데이트하다가, 업데이트 된 데이터가 캐시에서 빠지게 될 때 메인 메모리를 업데이트



## 4. 결론

* 캐싱은 전체적인 처리 속도를 향상
* 캐싱은 복사본을 사용하는 것이므로 일관성 유지에 유의

# Reference

[[10분 테코톡] 파피의 Cashing(캐싱)](https://www.youtube.com/watch?v=JBFT4KyEvoY)